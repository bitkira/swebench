# SWE-bench ä¸‰é˜¶æ®µå¯¹åº”çš„ä»£ç æ–‡ä»¶

## ğŸ“‹ æ€»è§ˆ

| é˜¶æ®µ | ä¸»è¦è„šæœ¬ | ä½ç½® | åŠŸèƒ½ |
|------|---------|------|------|
| é˜¶æ®µ1: æ•°æ®å‡†å¤‡ | `create_text_dataset.py` | `swebench/inference/make_datasets/` | ç”Ÿæˆå®Œæ•´ prompt |
| é˜¶æ®µ2: æ¨ç†ç”Ÿæˆ | `run_api.py` / `run_llama.py` / `run_live.py` | `swebench/inference/` | è°ƒç”¨æ¨¡å‹ç”Ÿæˆ patch |
| é˜¶æ®µ3: è¯„ä¼°æµ‹è¯• | `run_evaluation.py` | `swebench/harness/` | åœ¨ Docker ä¸­æµ‹è¯• patch |

---

## ğŸ”· é˜¶æ®µ 1: æ•°æ®å‡†å¤‡

### æ ¸å¿ƒæ–‡ä»¶ç»“æ„
```
swebench/inference/make_datasets/
â”œâ”€â”€ create_text_dataset.py      â† ä¸»å…¥å£è„šæœ¬
â”œâ”€â”€ create_instance.py          â† prompt æ ¼å¼åŒ–å‡½æ•°
â”œâ”€â”€ bm25_retrieval.py           â† BM25 æ£€ç´¢å®ç°
â”œâ”€â”€ tokenize_dataset.py         â† åˆ†è¯å™¨å·¥å…·
â””â”€â”€ utils.py                    â† è¾…åŠ©å‡½æ•°
```

### 1.1 ä¸»å…¥å£: `create_text_dataset.py`

**è·¯å¾„**: `swebench/inference/make_datasets/create_text_dataset.py`

**å…³é”®ä»£ç **:
```python
# ç¬¬ 114-254 è¡Œ: main() å‡½æ•°
def main(
    dataset_name_or_path,
    splits,
    file_source,          # oracle/bm25/all
    retrieval_file,
    prompt_style,         # style-2/style-3/...
    k,
    max_context_len,
    tokenizer_name,
    ...
):
    # 1. åŠ è½½åŸå§‹æ•°æ®é›†
    dataset = load_dataset(dataset_name_or_path)

    # 2. å¤„ç†æ¯ä¸ª split
    for split in splits:
        split_instances = {x["instance_id"]: x for x in dataset[split]}

        # 3. è°ƒç”¨æ ¸å¿ƒå‡½æ•°: æ·»åŠ  text_inputs (prompt)
        add_text_inputs(
            split_instances,
            retrieval_file=retrieval_file,
            k=k,
            prompt_style=prompt_style,
            file_source=file_source,          # â† å†³å®šå¦‚ä½•è·å–ä»£ç 
            max_context_len=max_context_len,
            tokenizer_name=tokenizer_name,
            progress_file=progress_file,
        )

    # 4. ä¿å­˜æ–°æ•°æ®é›†
    final_dataset.save_to_disk(output_file)
```

**å‘½ä»¤è¡Œä½¿ç”¨**:
```bash
python -m swebench.inference.make_datasets.create_text_dataset \
    --dataset_name_or_path princeton-nlp/SWE-bench_Lite \
    --file_source bm25 \
    --retrieval_file ./bm25.jsonl \
    --k 10 \
    --prompt_style style-3 \
    --output_dir ./prepared_dataset
```

---

### 1.2 æ ¸å¿ƒé€»è¾‘: `create_instance.py`

**è·¯å¾„**: `swebench/inference/make_datasets/create_instance.py`

#### å…³é”®å‡½æ•° 1: `add_text_inputs()`
```python
# ç¬¬ 340-495 è¡Œ
def add_text_inputs(
    instances,
    retrieval_file,
    k,
    prompt_style,
    file_source,              # oracle/bm25/all
    max_context_len=None,
    tokenizer_name=None,
    progress_file=None,
):
    """å¤„ç†å®ä¾‹å¹¶ç”Ÿæˆ text_inputs (å®Œæ•´ prompt)"""

    # 1. å¦‚æœä½¿ç”¨ BM25,æ·»åŠ æ£€ç´¢ç»“æœ
    if file_source in {"bm25"}:
        add_retrieval_results(instances, retrieval_file, k, file_source)

    # 2. å¯¹æ¯ä¸ªå®ä¾‹å¤„ç†
    for instance_id, instance in instances.items():
        with AutoContextManager(instance, root_dir) as cm:
            # 2.1 è·å– README æ–‡ä»¶
            readmes = cm.get_readme_files()
            processed_instance["readmes"] = ingest_files(readmes)

            # 2.2 æ ¹æ® file_source è·å–ä»£ç æ–‡ä»¶
            if file_source == "oracle":
                # ä»æ­£ç¡®ç­”æ¡ˆçš„ patch ä¸­æå–æ–‡ä»¶
                processed_instance["file_contents"] = ingest_files(
                    get_oracle_filenames(processed_instance)
                )
            elif file_source == "bm25":
                # ä½¿ç”¨ BM25 æ£€ç´¢çš„æ–‡ä»¶
                processed_instance["file_contents"] = ingest_files(
                    [x["docid"] for x in processed_instance["hits"]]
                )
            elif file_source == "all":
                # åŒ…å«æ‰€æœ‰æ–‡ä»¶
                processed_instance["file_contents"] = (
                    ingest_directory_contents(cm.repo_path)
                )

            # 2.3 å¦‚æœæœ‰ token é™åˆ¶,æŒ‰ token æ•°ç­›é€‰æ–‡ä»¶
            if max_context_len is not None:
                # é€ä¸ªæ·»åŠ æ–‡ä»¶,ç›´åˆ°è¾¾åˆ° token ä¸Šé™
                for filename in files:
                    tokens = tokenizer_func(file_content, tokenizer)
                    if current_tokens + len(tokens) < max_context_len:
                        include_files.append(filename)
                        current_tokens += len(tokens)

            # 2.4 ç”Ÿæˆæœ€ç»ˆçš„ text_inputs (å®Œæ•´ prompt)
            processed_instance["text_inputs"] = PROMPT_FUNCTIONS[prompt_style](
                processed_instance
            )

            # 2.5 ä¿å­˜åˆ°æ–‡ä»¶
            progress_file_handle.write(json.dumps(processed_instance) + "\n")
```

#### å…³é”®å‡½æ•° 2: Prompt æ ¼å¼åŒ–å‡½æ•°

**`prompt_style_3()` - æ¨èçš„ prompt æ ¼å¼** (ç¬¬ 221-256 è¡Œ)
```python
def prompt_style_3(instance):
    """ç”Ÿæˆ style-3 æ ¼å¼çš„ prompt"""
    premise = "You will be provided with a partial code base and an issue statement..."

    # æ ¼å¼åŒ– README
    readmes_text = make_code_text(instance["readmes"])

    # æ ¼å¼åŒ–ä»£ç æ–‡ä»¶(å¸¦è¡Œå·)
    code_text = make_code_text(instance["file_contents"])

    # ç»„è£…æœ€ç»ˆ prompt
    final_text = [
        premise,
        "<issue>",
        instance["problem_statement"],    # â† é—®é¢˜æè¿°
        "</issue>",
        "",
        "<code>",
        readmes_text,                     # â† README å†…å®¹
        code_text,                        # â† ç›¸å…³ä»£ç æ–‡ä»¶
        "</code>",
        "",
        "Here is an example of a patch file...",
        "<patch>",
        PATCH_EXAMPLE,                    # â† patch æ ¼å¼ç¤ºä¾‹
        "</patch>",
        "",
        "I need you to solve the provided issue...",
        "Respond below:",
    ]

    return "\n".join(final_text)
```

**`make_code_text()` - ä»£ç æ ¼å¼åŒ–** (ç¬¬ 127-136 è¡Œ)
```python
def make_code_text(files_dict, add_line_numbers=True):
    """å°†ä»£ç æ–‡ä»¶æ ¼å¼åŒ–ä¸ºå¸¦è¡Œå·çš„æ–‡æœ¬"""
    all_text = ""
    for filename, contents in sorted(files_dict.items()):
        all_text += f"[start of {filename}]\n"

        # æ·»åŠ è¡Œå·
        if add_line_numbers:
            for ix, line in enumerate(contents.split("\n"), start=1):
                all_text += f"{ix} {line}\n"
        else:
            all_text += contents

        all_text += f"[end of {filename}]\n"

    return all_text.strip("\n")
```

**`get_oracle_filenames()` - Oracle æ¨¡å¼** (ç¬¬ 326-337 è¡Œ)
```python
def get_oracle_filenames(instance):
    """ä»æ­£ç¡®çš„ patch ä¸­æå–è¢«ä¿®æ”¹çš„æ–‡ä»¶å"""
    source_files = {
        patch_file.source_file.split("a/", 1)[-1]
        for patch_file in unidiff.PatchSet(instance["patch"])
    }
    return source_files
```

**å…¶ä»– prompt æ ·å¼**:
- `prompt_style_2()`: ç¬¬ 165-190 è¡Œ
- `full_file_gen()`: ç¬¬ 259-284 è¡Œ
- `style_2_edits_only()`: ç¬¬ 193-218 è¡Œ

**PROMPT_FUNCTIONS å­—å…¸** (ç¬¬ 296-301 è¡Œ):
```python
PROMPT_FUNCTIONS = {
    "style-2": prompt_style_2,
    "style-3": prompt_style_3,
    "full_file_gen": full_file_gen,
    "style-2-edits-only": prompt_style_2_edits_only,
}
```

---

### 1.3 BM25 æ£€ç´¢: `bm25_retrieval.py`

**è·¯å¾„**: `swebench/inference/make_datasets/bm25_retrieval.py`

**å…³é”®å‡½æ•°**:
```python
# make_index(): æ„å»º BM25 ç´¢å¼•
def make_index(repo_dir, query, commit, document_encoding_func, ...):
    """ä¸ºä»“åº“æ„å»º BM25 ç´¢å¼•"""
    # 1. è·å–æ‰€æœ‰ä»£ç æ–‡ä»¶
    # 2. ç¼–ç ä¸ºæ–‡æ¡£
    # 3. æ„å»º Pyserini ç´¢å¼•
    # 4. è¿”å›ç´¢å¼•è·¯å¾„

# search(): æ£€ç´¢ç›¸å…³æ–‡ä»¶
def search(instance, index_dir):
    """ä½¿ç”¨ BM25 æ£€ç´¢æœ€ç›¸å…³çš„æ–‡ä»¶"""
    # 1. åŠ è½½ç´¢å¼•
    # 2. ç”¨ problem_statement ä½œä¸ºæŸ¥è¯¢
    # 3. è¿”å› Top-K æ–‡ä»¶
```

**å‘½ä»¤è¡Œä½¿ç”¨**:
```bash
python -m swebench.inference.make_datasets.bm25_retrieval \
    --dataset_name princeton-nlp/SWE-bench_Lite \
    --output_file ./bm25_results.jsonl
```

---

### 1.4 åˆ†è¯å·¥å…·: `tokenize_dataset.py`

**è·¯å¾„**: `swebench/inference/make_datasets/tokenize_dataset.py`

**å…³é”®ä»£ç ** (ç¬¬ 31-34 è¡Œ):
```python
TOKENIZER_FUNCS = {
    "cl100k": (tiktoken.get_encoding("cl100k_base"), cl100k),
    "llama": (LlamaTokenizer.from_pretrained("togethercomputer/LLaMA-2-7B-32K"), llama),
}
```

---

## ğŸ”· é˜¶æ®µ 2: æ¨ç†ç”Ÿæˆ

### æ ¸å¿ƒæ–‡ä»¶ç»“æ„
```
swebench/inference/
â”œâ”€â”€ run_api.py          â† API æ¨¡å‹æ¨ç† (GPT/Claude)
â”œâ”€â”€ run_llama.py        â† æœ¬åœ° LLaMA æ¨¡å‹æ¨ç†
â””â”€â”€ run_live.py         â† å®æ—¶ GitHub issue æ¨ç†
```

### 2.1 API æ¨ç†: `run_api.py`

**è·¯å¾„**: `swebench/inference/run_api.py`

**å…³é”®ä»£ç **:

#### main() å‡½æ•° (ç¬¬ 442-508 è¡Œ)
```python
def main(
    dataset_name_or_path,
    split,
    model_name_or_path,    # gpt-4, claude-2, etc.
    output_dir,
    model_args,
    max_cost,
):
    # 1. åŠ è½½å‡†å¤‡å¥½çš„æ•°æ®é›† (é˜¶æ®µ1çš„è¾“å‡º)
    dataset = load_dataset(dataset_name_or_path)[split]

    # 2. æŒ‰è¾“å…¥é•¿åº¦æ’åº
    lens = np.array(list(map(len, dataset["text"])))
    dataset = dataset.select(np.argsort(lens))

    # 3. è°ƒç”¨æ¨ç†å‡½æ•°
    if model_name_or_path.startswith("claude"):
        anthropic_inference(dataset, model_name_or_path, output_file, ...)
    elif model_name_or_path.startswith("gpt"):
        openai_inference(dataset, model_name_or_path, output_file, ...)
```

#### OpenAI æ¨ç† (ç¬¬ 174-243 è¡Œ)
```python
def openai_inference(test_dataset, model_name_or_path, output_file, ...):
    """ä½¿ç”¨ OpenAI API è¿è¡Œæ¨ç†"""

    # 1. è®¾ç½® API key
    openai.api_key = os.environ.get("OPENAI_API_KEY")

    # 2. å¯¹æ¯ä¸ªå®ä¾‹æ¨ç†
    for datum in tqdm(test_dataset):
        instance_id = datum["instance_id"]

        # 3. è¯»å–å‡†å¤‡å¥½çš„ prompt
        text_prompt = f"{datum['text']}\n\n"

        # 4. è°ƒç”¨ OpenAI API
        response, cost = call_chat(
            model_name_or_path,
            text_prompt,              # â† ä½¿ç”¨é˜¶æ®µ1ç”Ÿæˆçš„ prompt
            use_azure=False,
            temperature=0.2,
            top_p=0.95,
        )

        # 5. æå–ç”Ÿæˆçš„å†…å®¹
        completion = response.choices[0].message.content

        # 6. ä»ç”Ÿæˆå†…å®¹ä¸­æå– patch
        model_patch = extract_diff(completion)

        # 7. ä¿å­˜ç»“æœ
        output_dict = {
            "instance_id": instance_id,
            "model_name_or_path": model_name_or_path,
            "full_output": completion,
            "model_patch": model_patch,    # â† æå–çš„ patch
        }
        print(json.dumps(output_dict), file=f, flush=True)
```

#### call_chat() - OpenAI API è°ƒç”¨ (ç¬¬ 114-159 è¡Œ)
```python
@retry(wait=wait_random_exponential(min=30, max=600), stop=stop_after_attempt(3))
def call_chat(model_name_or_path, inputs, use_azure, temperature, top_p, **model_args):
    """è°ƒç”¨ OpenAI API"""

    # 1. åˆ†å‰² system message å’Œ user message
    system_messages = inputs.split("\n", 1)[0]
    user_message = inputs.split("\n", 1)[1]

    # 2. è°ƒç”¨ API
    response = openai.chat.completions.create(
        model=model_name_or_path,
        messages=[
            {"role": "system", "content": system_messages},
            {"role": "user", "content": user_message},    # â† å‘é€ prompt
        ],
        temperature=temperature,
        top_p=top_p,
        **model_args,
    )

    # 3. è®¡ç®—è´¹ç”¨
    input_tokens = response.usage.prompt_tokens
    output_tokens = response.usage.completion_tokens
    cost = calc_cost(response.model, input_tokens, output_tokens)

    return response, cost
```

#### Anthropic æ¨ç† (ç¬¬ 323-403 è¡Œ)
```python
def anthropic_inference(test_dataset, model_name_or_path, output_file, ...):
    """ä½¿ç”¨ Anthropic API è¿è¡Œæ¨ç†"""

    # ç±»ä¼¼ OpenAI,ä½†ä½¿ç”¨ Anthropic API
    api_key = os.environ.get("ANTHROPIC_API_KEY")
    anthropic = Anthropic(api_key=api_key)

    for datum in tqdm(test_dataset):
        # ä½¿ç”¨ datum["text"] ä½œä¸º prompt
        text_inputs = f"{datum['text']}\n"

        # è°ƒç”¨ Anthropic API
        completion, cost = call_anthropic(
            text_inputs,
            anthropic,
            model_name_or_path,
            temperature=0.2,
            top_p=0.95,
        )

        # æå– patch å¹¶ä¿å­˜
        ...
```

**å‘½ä»¤è¡Œä½¿ç”¨**:
```bash
# OpenAI
export OPENAI_API_KEY="sk-xxx"
python -m swebench.inference.run_api \
    --dataset_name_or_path ./prepared_dataset \
    --model_name_or_path gpt-4 \
    --output_dir ./predictions

# Anthropic
export ANTHROPIC_API_KEY="sk-xxx"
python -m swebench.inference.run_api \
    --dataset_name_or_path ./prepared_dataset \
    --model_name_or_path claude-3-opus-20240229 \
    --output_dir ./predictions
```

---

### 2.2 æœ¬åœ° LLaMA æ¨ç†: `run_llama.py`

**è·¯å¾„**: `swebench/inference/run_llama.py`

**å…³é”®ä»£ç **:

#### main() å‡½æ•° (ç¬¬ 360-424 è¡Œ)
```python
def main(
    model_name_or_path,
    peft_path,
    dataset_path,
    split,
    temperature,
    top_p,
    output_dir,
    ...
):
    # 1. åŠ è½½æ¨¡å‹
    model = load_model(model_name_or_path, peft_path)
    tokenizer = load_tokenizer(model_name_or_path)

    # 2. åŠ è½½æ•°æ®
    dataset = load_data(dataset_path, split, tokenizer, ...)

    # 3. ç”Ÿæˆ
    with open(output_file, "a") as f:
        generate(
            model=model,
            dataset=dataset,
            tokenizer=tokenizer,
            temperature=temperature,
            top_p=top_p,
            fileobj=f,
            ...
        )
```

#### generate() - ç”Ÿæˆå‡½æ•° (ç¬¬ 246-335 è¡Œ)
```python
def generate(model, dataset, tokenizer, temperature, top_p, fileobj, ...):
    """ä½¿ç”¨ LLaMA æ¨¡å‹ç”Ÿæˆ patch"""

    for instance in tqdm(dataset):
        # 1. å‡†å¤‡ input_ids (å·²ç»åœ¨ load_data æ—¶ tokenize)
        input_ids = torch.tensor([instance["input_ids"]], device=model.device)

        # 2. ç”Ÿæˆ
        output = model.generate(
            input_ids=input_ids,
            temperature=1.0 if temperature == 0 else temperature,
            top_p=top_p,
            do_sample=False if temperature == 0 else True,
            max_new_tokens=200,
            stopping_criteria=stopping_criteria,
        )

        # 3. è§£ç 
        output_text = tokenizer.decode(output[0], skip_special_tokens=False)

        # 4. æå– diff
        diff = extract_diff(output_text)

        # 5. ä¿å­˜
        result = {
            "instance_id": instance["instance_id"],
            "full_output": output_text,
            "model_patch": diff,
            "model_name_or_path": model_name_or_path,
        }
        print(json.dumps(result), file=fileobj, flush=True)
```

**å‘½ä»¤è¡Œä½¿ç”¨**:
```bash
python -m swebench.inference.run_llama \
    --dataset_path ./prepared_dataset \
    --model_name_or_path princeton-nlp/SWE-Llama-13b \
    --output_dir ./predictions \
    --temperature 0
```

---

### 2.3 å®æ—¶æ¨ç†: `run_live.py`

**è·¯å¾„**: `swebench/inference/run_live.py`

**å…³é”®ä»£ç ** (ç¬¬ 173-260 è¡Œ):
```python
def main(model_name, prompt_style, issue_url, ...):
    """å¯¹å®æ—¶ GitHub issue è¿è¡Œæ¨ç†"""

    # 1. è§£æ issue URL
    owner, repo, issue_num = parse_issue_url(issue_url)

    # 2. è·å– issue å†…å®¹
    problem_statement = get_problem_statement(owner, repo, issue_num, gh)

    # 3. åˆ›å»ºå®ä¾‹ (åŒ…å«æ£€ç´¢å’Œ prompt ç”Ÿæˆ)
    instance = make_instance(
        owner, repo, problem_statement, commit, root_dir,
        tokenizer, prompt_style, max_context_len, ...
    )

    # 4. è°ƒç”¨æ¨¡å‹
    if model_name.startswith("gpt"):
        response, _ = call_chat(model_name, instance["text_inputs"], ...)
        completion = response.choices[0].message.content
    else:
        # Anthropic
        completion = call_anthropic(instance["text_inputs"], ...)

    # 5. æå– patch å¹¶ä¿å­˜
    model_patch = extract_diff(completion)
```

---

## ğŸ”· é˜¶æ®µ 3: è¯„ä¼°æµ‹è¯•

### æ ¸å¿ƒæ–‡ä»¶ç»“æ„
```
swebench/harness/
â”œâ”€â”€ run_evaluation.py       â† è¯„ä¼°ä¸»è„šæœ¬
â”œâ”€â”€ docker_build.py         â† Docker é•œåƒæ„å»º
â”œâ”€â”€ docker_utils.py         â† Docker æ“ä½œå·¥å…·
â”œâ”€â”€ grading.py              â† è¯„åˆ†é€»è¾‘
â””â”€â”€ test_spec/              â† æµ‹è¯•è§„èŒƒç”Ÿæˆ
    â”œâ”€â”€ test_spec.py
    â””â”€â”€ create_scripts.py
```

### 3.1 è¯„ä¼°ä¸»è„šæœ¬: `run_evaluation.py`

**è·¯å¾„**: `swebench/harness/run_evaluation.py`

**å…³é”®ä»£ç **:

#### main() å‡½æ•° (ç®€åŒ–ç‰ˆé€»è¾‘)
```python
def main(
    dataset_name,
    predictions_path,    # é˜¶æ®µ2ç”Ÿæˆçš„ JSONL æ–‡ä»¶
    max_workers,
    run_id,
    ...
):
    # 1. åŠ è½½æ•°æ®é›†
    dataset = load_dataset(dataset_name)

    # 2. åŠ è½½æ¨¡å‹é¢„æµ‹
    predictions = load_predictions(predictions_path)

    # 3. å¯¹æ¯ä¸ªé¢„æµ‹è¿è¡Œè¯„ä¼°
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = []
        for prediction in predictions:
            future = executor.submit(
                run_instance_evaluation,
                prediction,
                dataset,
                run_id,
            )
            futures.append(future)

        # 4. æ”¶é›†ç»“æœ
        results = [f.result() for f in futures]

    # 5. ç”ŸæˆæŠ¥å‘Š
    generate_report(results, run_id)
```

#### run_instance_evaluation() - å•å®ä¾‹è¯„ä¼°
```python
def run_instance_evaluation(prediction, dataset, run_id):
    """è¯„ä¼°å•ä¸ªå®ä¾‹"""

    # 1. è·å–å¯¹åº”çš„æ•°æ®é›†å®ä¾‹
    instance = get_instance(dataset, prediction["instance_id"])

    # 2. æ„å»º Docker å®¹å™¨
    container = build_container(instance)

    # 3. åœ¨å®¹å™¨ä¸­åº”ç”¨ patch
    apply_result = apply_patch_in_container(
        container,
        prediction["model_patch"]    # â† æ¨¡å‹ç”Ÿæˆçš„ patch
    )

    # 4. è¿è¡Œæµ‹è¯•
    if apply_result.success:
        test_result = run_tests_in_container(
            container,
            instance["test_patch"],
            instance["FAIL_TO_PASS"],
            instance["PASS_TO_PASS"],
        )
    else:
        test_result = {"resolved": False, "error": "patch failed to apply"}

    # 5. åˆ¤æ–­æ˜¯å¦è§£å†³
    resolved = (
        apply_result.success and
        all_fail_to_pass_tests_passed(test_result) and
        all_pass_to_pass_tests_passed(test_result)
    )

    # 6. è¿”å›ç»“æœ
    return {
        "instance_id": prediction["instance_id"],
        "resolved": resolved,
        "test_result": test_result,
        "apply_result": apply_result,
    }
```

---

### 3.2 è¯„åˆ†é€»è¾‘: `grading.py`

**è·¯å¾„**: `swebench/harness/grading.py`

**å…³é”®å‡½æ•°**:
```python
def get_eval_report(
    test_spec,
    prediction,
    log_path,
    include_tests_status=False,
):
    """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""

    # 1. è§£ææµ‹è¯•æ—¥å¿—
    log_content = open(log_path).read()

    # 2. æå–æµ‹è¯•ç»“æœ
    fail_to_pass_results = parse_log_for_tests(
        log_content,
        test_spec["FAIL_TO_PASS"]
    )
    pass_to_pass_results = parse_log_for_tests(
        log_content,
        test_spec["PASS_TO_PASS"]
    )

    # 3. åˆ¤æ–­æ˜¯å¦è§£å†³
    resolved = (
        all(result == "PASSED" for result in fail_to_pass_results) and
        all(result == "PASSED" for result in pass_to_pass_results)
    )

    return {
        "instance_id": test_spec["instance_id"],
        "resolved": resolved,
        "fail_to_pass": fail_to_pass_results,
        "pass_to_pass": pass_to_pass_results,
    }
```

---

### 3.3 Docker å·¥å…·: `docker_utils.py`

**è·¯å¾„**: `swebench/harness/docker_utils.py`

**å…³é”®å‡½æ•°**:
```python
def run_docker_container(container_name, image_name, timeout=3600):
    """è¿è¡Œ Docker å®¹å™¨"""
    client = docker.from_env()
    container = client.containers.run(
        image_name,
        name=container_name,
        detach=True,
        ...
    )
    return container

def exec_run_with_timeout(container, command, timeout=3600):
    """åœ¨å®¹å™¨ä¸­æ‰§è¡Œå‘½ä»¤"""
    exit_code, output = container.exec_run(
        command,
        workdir="/testbed",
    )
    return exit_code, output
```

**å‘½ä»¤è¡Œä½¿ç”¨**:
```bash
python -m swebench.harness.run_evaluation \
    --dataset_name princeton-nlp/SWE-bench_Lite \
    --predictions_path ./predictions/gpt-4__predictions.jsonl \
    --max_workers 8 \
    --run_id gpt4_eval
```

---

## ğŸ“Š å®Œæ•´ä»£ç è°ƒç”¨é“¾

### é˜¶æ®µ 1 è°ƒç”¨é“¾
```
create_text_dataset.py:main()
    â†“
create_text_dataset.py:add_text_inputs()
    â†“
create_instance.py:add_text_inputs()
    â”œâ”€â†’ bm25_retrieval.py:search()            [å¦‚æœ file_source=bm25]
    â”œâ”€â†’ create_instance.py:get_oracle_filenames() [å¦‚æœ file_source=oracle]
    â”œâ”€â†’ utils.py:ingest_directory_contents()   [å¦‚æœ file_source=all]
    â”œâ”€â†’ tokenize_dataset.py:TOKENIZER_FUNCS   [å¦‚æœæœ‰ max_context_len]
    â””â”€â†’ create_instance.py:PROMPT_FUNCTIONS[prompt_style]()
        â”œâ”€â†’ prompt_style_3()
        â””â”€â†’ make_code_text()
```

### é˜¶æ®µ 2 è°ƒç”¨é“¾ (API)
```
run_api.py:main()
    â†“
run_api.py:openai_inference() æˆ– anthropic_inference()
    â†“
run_api.py:call_chat() æˆ– call_anthropic()
    â†“
OpenAI/Anthropic API
    â†“
utils.py:extract_diff()
```

### é˜¶æ®µ 2 è°ƒç”¨é“¾ (LLaMA)
```
run_llama.py:main()
    â†“
run_llama.py:load_model()
    â†“
run_llama.py:load_data()
    â†“
run_llama.py:generate()
    â†“
utils.py:extract_diff()
```

### é˜¶æ®µ 3 è°ƒç”¨é“¾
```
run_evaluation.py:main()
    â†“
run_evaluation.py:run_instance_evaluation() [å¹¶è¡Œæ‰§è¡Œ]
    â†“
docker_build.py:build_container()
    â†“
docker_utils.py:run_docker_container()
    â†“
docker_utils.py:exec_run_with_timeout() [åº”ç”¨ patch]
    â†“
docker_utils.py:exec_run_with_timeout() [è¿è¡Œæµ‹è¯•]
    â†“
grading.py:get_eval_report()
```

---

## ğŸ¯ å¿«é€ŸæŸ¥æ‰¾ä»£ç çš„æŠ€å·§

### æ–¹æ³• 1: æŒ‰åŠŸèƒ½æŸ¥æ‰¾
```bash
# æŸ¥æ‰¾ prompt ç›¸å…³
grep -r "def prompt_style" swebench/inference/make_datasets/

# æŸ¥æ‰¾ API è°ƒç”¨
grep -r "openai.chat.completions" swebench/inference/

# æŸ¥æ‰¾è¯„ä¼°é€»è¾‘
grep -r "def get_eval_report" swebench/harness/
```

### æ–¹æ³• 2: æŒ‰æ–‡ä»¶ç»“æ„
```
é˜¶æ®µ1: swebench/inference/make_datasets/*.py
é˜¶æ®µ2: swebench/inference/run_*.py
é˜¶æ®µ3: swebench/harness/*.py
```

### æ–¹æ³• 3: ä»å‘½ä»¤è¡Œå…¥å£è¿½è¸ª
```bash
# æ‰¾åˆ°å‘½ä»¤å¯¹åº”çš„æ–‡ä»¶
python -m swebench.inference.make_datasets.create_text_dataset
# â†’ swebench/inference/make_datasets/create_text_dataset.py

python -m swebench.inference.run_api
# â†’ swebench/inference/run_api.py

python -m swebench.harness.run_evaluation
# â†’ swebench/harness/run_evaluation.py
```

---

**åˆ›å»ºæ—¶é—´**: 2025-10-06
**ç»´æŠ¤è€…**: SWE-bench Team
